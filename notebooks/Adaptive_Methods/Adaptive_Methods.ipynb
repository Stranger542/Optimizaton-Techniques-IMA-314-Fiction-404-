{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e1d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory (e.g., .../Optimization_Project/notebooks)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (e.g., .../Optimization_Project/)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    print(f\"Added '{parent_dir}' to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ef8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import base classes and helpers\n",
    "from utils.base import Function\n",
    "from utils.plot_helpers import plot_loss_curves, plot_contour_comparison\n",
    "\n",
    "# Import test functions\n",
    "from utils.test_functions import (\n",
    "    Rosenbrock, \n",
    "    Quadratic, \n",
    "    generate_linear_regression_data, \n",
    "    linear_regression_loss, \n",
    "    linear_regression_gradient\n",
    ")\n",
    "\n",
    "# Import the optimizers we are testing\n",
    "from optimizers.adaptive import (\n",
    "    Adagrad,\n",
    "    RMSProp,\n",
    "    Adam\n",
    ")\n",
    "\n",
    "# Magic command for plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09858078",
   "metadata": {},
   "source": [
    "## 1. Contour Plot: Adaptive Methods on Rosenbrock\n",
    "\n",
    "This plot compares the optimization paths of `Adagrad`, `RMSProp`, and `Adam` on the 2D Rosenbrock function.\n",
    "\n",
    "Adaptive methods adjust the learning rate for each parameter automatically, which often makes them less sensitive to the initial learning rate (`alpha`) and more effective at navigating difficult landscapes like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define Optimizers ---\n",
    "\n",
    "# All three need a large alpha to overcome the initial massive gradient\n",
    "adagrad = Adagrad(alpha=0.1)\n",
    "rmsprop = RMSProp(alpha=0.1) # Was 0.001, which caused stagnation\n",
    "adam = Adam(alpha=0.1)     # Was 0.001, which would also stagnate\n",
    "\n",
    "# --- 2. Define Problem ---\n",
    "start_point = np.array([-1.5, -1.0])\n",
    "\n",
    "# --- 3. Run Optimizations ---\n",
    "print(\"Running Adagrad...\")\n",
    "sol_ada, history_ada = adagrad.optimize(\n",
    "    x=start_point.copy(),\n",
    "    func_callback=Rosenbrock,\n",
    "    grad_func_callback=Rosenbrock.grad,\n",
    "    hessian_func_callback=Rosenbrock.hessian,\n",
    "    is_plot=True \n",
    ")\n",
    "print(f\"Found solution: {sol_ada} in {adagrad.num_iter} iterations.\\n\")\n",
    "\n",
    "\n",
    "print(\"Running RMSProp...\")\n",
    "sol_rms, history_rms = rmsprop.optimize(\n",
    "    x=start_point.copy(),\n",
    "    func_callback=Rosenbrock,\n",
    "    grad_func_callback=Rosenbrock.grad,\n",
    "    hessian_func_callback=Rosenbrock.hessian,\n",
    "    is_plot=True\n",
    ")\n",
    "print(f\"Found solution: {sol_rms} in {rmsprop.num_iter} iterations.\\n\")\n",
    "\n",
    "print(\"Running Adam...\")\n",
    "sol_adam, history_adam = adam.optimize(\n",
    "    x=start_point.copy(),\n",
    "    func_callback=Rosenbrock,\n",
    "    grad_func_callback=Rosenbrock.grad,\n",
    "    hessian_func_callback=Rosenbrock.hessian,\n",
    "    is_plot=True\n",
    ")\n",
    "print(f\"Found solution: {sol_adam} in {adam.num_iter} iterations.\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Plot Comparison ---\n",
    "plot_contour_comparison(\n",
    "    func_callable=Rosenbrock,\n",
    "    histories={\n",
    "        f\"Adagrad (alpha=0.1)\": history_ada,\n",
    "        f\"RMSProp (alpha=0.01)\": history_rms,\n",
    "        f\"Adam (alpha=0.01)\": history_adam\n",
    "    },\n",
    "    x_range=(-2, 2),\n",
    "    y_range=(-1.5, 2.5), # Zoom in on the Rosenbrock valley\n",
    "    title=\"Adaptive Optimizers on Rosenbrock\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5313ae77",
   "metadata": {},
   "source": [
    "## 2. Application Plot: Adaptive Methods on Linear Regression\n",
    "\n",
    "This plot compares the convergence speed of the adaptive optimizers on our linear regression problem. \n",
    "\n",
    "Since `Adagrad`, `RMSProp`, and `Adam` are all *batch* methods (they use the full gradient), we will compare them against each other using the same batch loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e63823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Generate Data ---\n",
    "N = 200  # Number of data points\n",
    "d = 1    # Number of features\n",
    "X_aug, Y, W_true = generate_linear_regression_data(N=N, d=d)\n",
    "\n",
    "print(f\"Data shape (X_aug): {X_aug.shape}\")\n",
    "print(f\"Data shape (Y): {Y.shape}\")\n",
    "print(f\"True weights (W_true): \\n{W_true}\\n\")\n",
    "\n",
    "# --- 2. Define Initial Weights ---\n",
    "W_initial_2D = np.random.randn(d + 1, 1)\n",
    "W_initial_1D = W_initial_2D.flatten() \n",
    "print(f\"Initial weights (W_initial): \\n{W_initial_2D}\\n\")\n",
    "\n",
    "# --- 3. Setup Callbacks for Optimizers ---\n",
    "# We create wrappers that pre-load the X and Y data.\n",
    "\n",
    "def batch_loss_wrapper(W_1D):\n",
    "    W_2D = W_1D.reshape(-1, 1)\n",
    "    return linear_regression_loss(W_2D, X_aug, Y)\n",
    "\n",
    "def batch_grad_wrapper(W_1D):\n",
    "    W_2D = W_1D.reshape(-1, 1)\n",
    "    return linear_regression_gradient(W_2D, X_aug, Y).flatten()\n",
    "\n",
    "# Create the Function object\n",
    "linear_reg_func = Function(\n",
    "    func=batch_loss_wrapper,\n",
    "    grad_func=batch_grad_wrapper,\n",
    "    name=\"Linear Regression MSE\"\n",
    ")\n",
    "\n",
    "# --- 4. Setup for Plotting ---\n",
    "def final_loss_plotter(W):\n",
    "    W_2D = W.reshape(-1, 1) # Ensure W is 2D\n",
    "    return linear_regression_loss(W_2D, X_aug, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Instantiate Optimizers ---\n",
    "# We use the same parameters as the contour plot\n",
    "adagrad_lr = Adagrad(alpha=0.1)\n",
    "rmsprop_lr = RMSProp(alpha=0.01)\n",
    "adam_lr = Adam(alpha=0.01)\n",
    "\n",
    "# --- 2. Run Optimizers ---\n",
    "\n",
    "print(\"Running Adagrad on Linear Regression...\")\n",
    "sol_ada_lr, hist_ada_lr = adagrad_lr.optimize(\n",
    "    x=W_initial_1D.copy(),\n",
    "    func_callback=linear_reg_func,\n",
    "    grad_func_callback=linear_reg_func.grad,\n",
    "    is_plot=True\n",
    ")\n",
    "print(f\"Adagrad done in {adagrad_lr.num_iter} iterations.\")\n",
    "\n",
    "\n",
    "print(\"\\nRunning RMSProp on Linear Regression...\")\n",
    "sol_rms_lr, hist_rms_lr = rmsprop_lr.optimize(\n",
    "    x=W_initial_1D.copy(),\n",
    "    func_callback=linear_reg_func,\n",
    "    grad_func_callback=linear_reg_func.grad,\n",
    "    is_plot=True\n",
    ")\n",
    "print(f\"RMSProp done in {rmsprop_lr.num_iter} iterations.\")\n",
    "\n",
    "\n",
    "print(\"\\nRunning Adam on Linear Regression...\")\n",
    "sol_adam_lr, hist_adam_lr = adam_lr.optimize(\n",
    "    x=W_initial_1D.copy(),\n",
    "    func_callback=linear_reg_func,\n",
    "    grad_func_callback=linear_reg_func.grad,\n",
    "    is_plot=True\n",
    ")\n",
    "print(f\"Adam done in {adam_lr.num_iter} iterations.\")\n",
    "\n",
    "# --- 3. Plot Loss Curves ---\n",
    "print(\"\\nGenerating loss curve comparison...\")\n",
    "plot_loss_curves(\n",
    "    histories={\n",
    "        \"Adagrad (alpha=0.1)\": hist_ada_lr,\n",
    "        \"RMSProp (alpha=0.01)\": hist_rms_lr,\n",
    "        \"Adam (alpha=0.01)\": hist_adam_lr\n",
    "    },\n",
    "    loss_func_callable=final_loss_plotter\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
