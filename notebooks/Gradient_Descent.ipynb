{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc49367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory (e.g., .../Optimization_Project/notebooks)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory (e.g., .../Optimization_Project/)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95340546",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'list' from 'typing' (c:\\Users\\sumit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\typing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Import base classes and helpers\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_loss_curves, plot_contour_comparison\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Import test functions\u001b[39;00m\n",
      "File \u001b[1;32md:\\Resume_Project\\Optimization\\utils\\base.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmplot3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes3D\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, Any\n\u001b[0;32m      9\u001b[0m EPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOptim\u001b[39;00m(ABC):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'list' from 'typing' (c:\\Users\\sumit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\typing.py)"
     ]
    }
   ],
   "source": [
    "#  notebooks/01_Gradient_Descent.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import base classes and helpers\n",
    "from utils.base import Function\n",
    "from utils.plot_helpers import plot_loss_curves, plot_contour_comparison\n",
    "\n",
    "# Import test functions\n",
    "from utils.test_functions import (\n",
    "    Rosenbrock, \n",
    "    Quadratic, \n",
    "    generate_linear_regression_data, \n",
    "    linear_regression_loss, \n",
    "    linear_regression_gradient\n",
    ")\n",
    "\n",
    "# Import the optimizers we are testing\n",
    "from optimizers.gradient_descent import (\n",
    "    GradientDescent, \n",
    "    StochasticGradientDescent, \n",
    "    MiniBatchGradientDescent\n",
    ")\n",
    "\n",
    "# Import a line search method for comparison\n",
    "from optimizers.line_search import BacktrackingLineSearch\n",
    "\n",
    "# Magic command for plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define Optimizers ---\n",
    "\n",
    "# Optimizer with a small, fixed step size\n",
    "# Rosenbrock is very sensitive, so a standard alpha=0.01 will explode.\n",
    "gd_fixed = GradientDescent(alpha=0.0001)\n",
    "\n",
    "# Optimizer with Backtracking Line Search\n",
    "# We create an instance of the line search optimizer first\n",
    "line_search_optim = BacktrackingLineSearch(alpha_init=1.0, beta=0.5, c1=1e-4)\n",
    "# We then pass the line search optimizer to GradientDescent\n",
    "gd_line_search = GradientDescent(alpha=1.0, alpha_optim=line_search_optim)\n",
    "\n",
    "# --- 2. Define Problem ---\n",
    "start_point = np.array([-1.5, -1.0])\n",
    "\n",
    "# --- 3. Run Optimizations ---\n",
    "print(\"Running Batch GD (Fixed Step)...\")\n",
    "# We call optimize() with is_plot=True to get the history\n",
    "sol_fixed, history_fixed = gd_fixed.optimize(\n",
    "    x=start_point.copy(),\n",
    "    func_callback=Rosenbrock,\n",
    "    grad_func_callback=Rosenbrock.grad,\n",
    "    hessian_func_callback=Rosenbrock.hessian,\n",
    "    is_plot=True \n",
    ")\n",
    "print(f\"Found solution: {sol_fixed} in {gd_fixed.num_iter} iterations.\\n\")\n",
    "\n",
    "\n",
    "print(\"Running Batch GD (Line Search)...\")\n",
    "sol_ls, history_ls = gd_line_search.optimize(\n",
    "    x=start_point.copy(),\n",
    "    func_callback=Rosenbrock,\n",
    "    grad_func_callback=Rosenbrock.grad,\n",
    "    hessian_func_callback=Rosenbrock.hessian,\n",
    "    is_plot=True\n",
    ")\n",
    "print(f\"Found solution: {sol_ls} in {gd_line_search.num_iter} iterations.\\n\")\n",
    "\n",
    "# --- 4. Plot Comparison ---\n",
    "plot_contour_comparison(\n",
    "    func_callable=Rosenbrock,\n",
    "    histories={\n",
    "        f\"GD (Fixed, alpha=0.0001)\": history_fixed,\n",
    "        f\"GD (Line Search)\": history_ls\n",
    "    },\n",
    "    x_range=(-2, 2),\n",
    "    y_range=(-1.5, 2.5), # Zoom in on the Rosenbrock valley\n",
    "    title=\"Batch GD: Fixed vs. Line Search on Rosenbrock\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24786db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Generate Data ---\n",
    "N = 200  # Number of data points\n",
    "d = 1    # Number of features\n",
    "X_aug, Y, W_true = generate_linear_regression_data(N=N, d=d)\n",
    "\n",
    "print(f\"Data shape (X_aug): {X_aug.shape}\")\n",
    "print(f\"Data shape (Y): {Y.shape}\")\n",
    "print(f\"True weights (W_true): \\n{W_true}\\n\")\n",
    "\n",
    "# --- 2. Define Initial Weights ---\n",
    "# We use a 2D column vector for SGD/MiniBatch\n",
    "W_initial_2D = np.random.randn(d + 1, 1)\n",
    "# We use a 1D vector for Batch GD (as required by the Function class)\n",
    "W_initial_1D = W_initial_2D.flatten() \n",
    "\n",
    "print(f\"Initial weights (W_initial): \\n{W_initial_2D}\\n\")\n",
    "\n",
    "# --- 3. Setup for Batch GD ---\n",
    "# Batch GD uses the `Function` class, which needs callbacks.\n",
    "# We create wrappers that pre-load the X and Y data.\n",
    "\n",
    "def batch_loss_wrapper(W_1D):\n",
    "    # The optimizer passes a 1D vector, so we reshape\n",
    "    W_2D = W_1D.reshape(-1, 1)\n",
    "    return linear_regression_loss(W_2D, X_aug, Y)\n",
    "\n",
    "def batch_grad_wrapper(W_1D):\n",
    "    # Reshape, compute gradient, and flatten\n",
    "    W_2D = W_1D.reshape(-1, 1)\n",
    "    return linear_regression_gradient(W_2D, X_aug, Y).flatten()\n",
    "\n",
    "# Create the Function object for Batch GD\n",
    "linear_reg_func = Function(\n",
    "    func=batch_loss_wrapper,\n",
    "    grad_func=batch_grad_wrapper,\n",
    "    name=\"Linear Regression MSE\"\n",
    ")\n",
    "\n",
    "# --- 4. Setup for Plotting ---\n",
    "# Create a single loss function for plot_loss_curves\n",
    "# This helper can handle both 1D and 2D weight vectors.\n",
    "def final_loss_plotter(W):\n",
    "    W_2D = W.reshape(-1, 1) # Ensure W is 2D\n",
    "    return linear_regression_loss(W_2D, X_aug, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Instantiate Optimizers ---\n",
    "\n",
    "# Batch GD (500 iterations, 1 update per iteration)\n",
    "gd = GradientDescent(alpha=0.1) \n",
    "\n",
    "# SGD (5 epochs, N updates per epoch)\n",
    "sgd = StochasticGradientDescent(alpha=0.01, n_epochs=5)\n",
    "\n",
    "# Mini-Batch (5 epochs, N/B updates per epoch)\n",
    "minibatch = MiniBatchGradientDescent(alpha=0.01, n_epochs=5, batch_size=32)\n",
    "\n",
    "# --- 2. Run Optimizers ---\n",
    "\n",
    "print(\"Running Batch GD...\")\n",
    "sol_gd, hist_gd = gd.optimize(\n",
    "    x=W_initial_1D.copy(),\n",
    "    func_callback=linear_reg_func,\n",
    "    grad_func_callback=linear_reg_func.grad,\n",
    "    is_plot=True\n",
    ")\n",
    "print(f\"Batch GD done in {gd.num_iter} iterations.\")\n",
    "\n",
    "\n",
    "print(\"\\nRunning SGD...\")\n",
    "# Note: SGD/MiniBatch optimizers override `optimize`\n",
    "# and take X, Y directly, as per our implementation.\n",
    "sol_sgd, hist_sgd = sgd.optimize(\n",
    "    x=W_initial_2D.copy(), # Pass 2D weights\n",
    "    X=X_aug,\n",
    "    Y=Y,\n",
    "    is_plot=True\n",
    ")\n",
    "print(f\"SGD done in {sgd.n_epochs} epochs.\")\n",
    "\n",
    "\n",
    "print(\"\\nRunning Mini-Batch GD...\")\n",
    "sol_mb, hist_mb = minibatch.optimize(\n",
    "    x=W_initial_2D.copy(), # Pass 2D weights\n",
    "    X=X_aug,\n",
    "    Y=Y,\n",
    "    is_plot=True\n",
    ")\n",
    "print(f\"Mini-Batch GD done in {minibatch.n_epochs} epochs.\")\n",
    "\n",
    "# --- 3. Plot Loss Curves ---\n",
    "plot_loss_curves(\n",
    "    histories={\n",
    "        \"Batch GD\": hist_gd,\n",
    "        \"SGD (5 epochs)\": hist_sgd,\n",
    "        \"Mini-Batch (5 epochs)\": hist_mb\n",
    "    },\n",
    "    loss_func_callable=final_loss_plotter\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
