{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import base classes and helpers\n",
    "from utils.base import Function\n",
    "from utils.plot_helpers import plot_loss_curves, plot_decision_boundary\n",
    "\n",
    "# Import test functions\n",
    "from utils.test_functions import (\n",
    "    generate_linear_regression_data, \n",
    "    linear_regression_loss,\n",
    "    generate_logistic_regression_data\n",
    ")\n",
    "\n",
    "# Import the optimizers\n",
    "from optimizers.gradient_descent import MiniBatchGradientDescent\n",
    "from optimizers.non_differentiable import SubGradientMethod\n",
    "\n",
    "# Import the models\n",
    "from models.linear_regression import LinearRegression, RidgeRegression, LassoRegression\n",
    "from models.logistic_regression import LogisticRegression\n",
    "\n",
    "# Magic command for plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d1d97",
   "metadata": {},
   "source": [
    "# 1. Linear & Ridge Regression (L2)\n",
    "\n",
    "First, we'll test `LinearRegression` and `RidgeRegression` on synthetic data. Both use differentiable loss functions, so we can train them with an optimizer like `MiniBatchGradientDescent`.\n",
    "\n",
    "We expect to see:\n",
    "- Both models converge.\n",
    "- The final weights for `RidgeRegression` will be smaller (\"shrunken\") than for standard `LinearRegression` due to the L2 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Generate Data ---\n",
    "X_lin, Y_lin, W_true_lin = generate_linear_regression_data(N=200, d=5)\n",
    "print(f\"Linear data shape (X): {X_lin.shape}\")\n",
    "print(f\"Linear data shape (Y): {Y_lin.shape}\")\n",
    "\n",
    "# --- 2. Instantiate Optimizer ---\n",
    "# We'll use the same optimizer for both\n",
    "optim_mb = MiniBatchGradientDescent(alpha=0.01, n_epochs=20, batch_size=32)\n",
    "\n",
    "# --- 3. Train Linear Regression ---\n",
    "lin_reg = LinearRegression()\n",
    "print(\"\\nTraining Standard Linear Regression...\")\n",
    "lin_reg.train(X_lin[:, 1:], Y_lin, optim_mb, is_plot=True) # Pass non-augmented X\n",
    "print(f\"Test Loss (MSE): {lin_reg.test(X_lin[:, 1:], Y_lin):.4f}\")\n",
    "\n",
    "# --- 4. Train Ridge Regression ---\n",
    "ridge_reg = RidgeRegression(alpha=0.5) # alpha is the regularization strength\n",
    "print(\"\\nTraining Ridge (L2) Regression...\")\n",
    "ridge_reg.train(X_lin[:, 1:], Y_lin, optim_mb, is_plot=True)\n",
    "print(f\"Test Loss (MSE): {ridge_reg.test(X_lin[:, 1:], Y_lin):.4f}\")\n",
    "\n",
    "# --- 5. Compare Weights ---\n",
    "print(\"\\n--- Weight Comparison ---\")\n",
    "print(f\"True Weights:\\n{W_true_lin.flatten()}\")\n",
    "print(f\"Linear Reg Weights:\\n{lin_reg.W.flatten()}\")\n",
    "print(f\"Ridge Reg Weights:\\n{ridge_reg.W.flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function for plotting\n",
    "# We pass the *raw* data, the models handle the bias augmentation\n",
    "def loss_plotter_lin(W):\n",
    "    return linear_regression_loss(W, X_lin, Y_lin)\n",
    "\n",
    "# Plot the loss curves\n",
    "plot_loss_curves(\n",
    "    histories={\n",
    "        \"Linear Regression\": lin_reg.history,\n",
    "        \"Ridge Regression (L2)\": ridge_reg.history\n",
    "    },\n",
    "    loss_func_callable=loss_plotter_lin\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e208be3",
   "metadata": {},
   "source": [
    "# 2. Lasso Regression (L1)\n",
    "\n",
    "Next, we'll test `LassoRegression`. This model has a non-differentiable L1 penalty, so we **must** train it with the `SubGradientMethod`.\n",
    "\n",
    "We expect to see:\n",
    "- [cite_start]The model converges (though the loss curve may be noisy [cite: 801-810]).\n",
    "- The final weights have some values that are exactly zero (sparsity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd85063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Instantiate Optimizer ---\n",
    "# We must use a sub-gradient optimizer\n",
    "# [cite_start]A diminishing step size is recommended [cite: 1536-1538]\n",
    "optim_subgrad = SubGradientMethod(alpha=0.1, n_iterations=1000, policy='diminishing')\n",
    "\n",
    "# --- 2. Train Lasso Regression ---\n",
    "lasso_reg = LassoRegression(alpha=0.1) # alpha is the regularization strength\n",
    "print(\"\\nTraining Lasso (L1) Regression...\")\n",
    "lasso_reg.train(X_lin[:, 1:], Y_lin, optim_subgrad, is_plot=True)\n",
    "print(f\"Test Loss (MSE): {lasso_reg.test(X_lin[:, 1:], Y_lin):.4f}\")\n",
    "\n",
    "# --- 3. Compare Weights ---\n",
    "print(\"\\n--- Weight Comparison (Lasso) ---\")\n",
    "print(f\"True Weights:\\n{W_true_lin.flatten()}\")\n",
    "print(f\"Lasso Reg Weights:\\n{lasso_reg.W.flatten()}\")\n",
    "print(\"\\nNotice how Lasso pushes some weights to zero!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ac894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve for Lasso\n",
    "# Note: The sub-gradient method is not a descent method,\n",
    "# so we plot the history of f_best, not the path.\n",
    "# For simplicity here, we plot the path, but expect it to be noisy.\n",
    "\n",
    "plot_loss_curves(\n",
    "    histories={\n",
    "        \"Lasso Regression (L1)\": lasso_reg.history\n",
    "    },\n",
    "    loss_func_callable=loss_plotter_lin\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e641b",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression (Classification)\n",
    "\n",
    "Finally, we'll test `LogisticRegression` on a 2D binary classification task. We'll use our new `generate_logistic_regression_data` function.\n",
    "\n",
    "We expect to see:\n",
    "- The model trains and achieves high accuracy.\n",
    "- We can plot a clear decision boundary separating the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Generate Data ---\n",
    "X_log, Y_log = generate_logistic_regression_data(N=300, d=2)\n",
    "\n",
    "# --- 2. Instantiate Optimizer ---\n",
    "optim_mb_log = MiniBatchGradientDescent(alpha=0.1, n_epochs=100, batch_size=32)\n",
    "\n",
    "# --- 3. Train Logistic Regression ---\n",
    "log_reg = LogisticRegression()\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "log_reg.train(X_log, Y_log, optim_mb_log, is_plot=True) # Pass non-augmented X\n",
    "print(f\"Test Accuracy: {log_reg.test(X_log, Y_log):.4f}\")\n",
    "\n",
    "# --- 4. Plot Decision Boundary ---\n",
    "# This uses the new helper function\n",
    "plot_decision_boundary(log_reg, X_log, Y_log, title=\"Logistic Regression Decision Boundary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function for plotting\n",
    "# This uses the model's internal (private) loss function\n",
    "def loss_plotter_log(W):\n",
    "    X_aug = log_reg._add_bias(X_log)\n",
    "    return log_reg._loss(W.reshape(-1, 1), X_aug, Y_log)\n",
    "\n",
    "# Plot the loss curves\n",
    "plot_loss_curves(\n",
    "    histories={\n",
    "        \"Logistic Regression\": log_reg.history\n",
    "    },\n",
    "    loss_func_callable=loss_plotter_log\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
