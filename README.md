# Optimization Algorithms from Scratch

A semester project implementation of various **mathematical optimization algorithms**.  
This repository contains the **core Python code** for each algorithm along with **Jupyter Notebooks** demonstrating, visualizing, and comparing their performance.

---

## Authors

- **Sumit Hulke** (2023BCD0026)  
- **Suraj Sanjay Harlekar** (2023BCD0038)
- **Aryan Patil** (2023BCD0047)

---

## About This Project

This project serves as an **educational exploration** into the field of **numerical optimization**.  
Based on the *Lecture 1â€“7* series, it implements foundational optimization algorithms **from scratch** using only **NumPy**.

The main objectives are to:
- Understand the internal working of optimization techniques.
- Analyze their behavior on test functions (e.g., simple quadratics, non-convex functions).
- Visualize their convergence trajectories.

The project separates:
- **Reusable algorithm logic** â†’ in `.py` files  
- **Experimental analysis & visualization** â†’ in `.ipynb` notebooks  

---

## Algorithms Implemented

### 1. First-Order Methods (Gradient-Based)
- **Gradient Descent (GD)** â€” Foundational batch optimization.  
- **Stochastic Gradient Descent (SGD)** â€” Processes one data point at a time.  
- **Mini-Batch SGD** â€” Balances speed and stability with small batches.

### 2. Momentum-Based Methods
- **Momentum GD** â€” Adds a velocity term to smooth updates.  
- **Nesterov Accelerated Gradient (NAG)** â€” A "look-ahead" version for faster convergence.

### 3. Adaptive Learning Rate Methods
- **Adagrad** â€” Adapts learning rate for each parameter.  
- **RMSProp** â€” Uses an exponentially weighted moving average of squared gradients.  
- **Adam** â€” Combines Momentum and RMSProp with bias correction.

### 4. Second-Order Methods
- **Newtonâ€™s Method** â€” Uses the Hessian for rapid convergence (one step for quadratics).  
- **Damped Newtonâ€™s Method** â€” Stabilizes updates when the Hessian isnâ€™t positive definite.

### 5. Quasi-Newton Methods
- **BFGS (Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno)** â€” Approximates the inverse Hessian efficiently.  
- **L-BFGS (Limited-memory BFGS)** â€” A memory-efficient version for high-dimensional problems using two-loop recursion.

### 6. Non-Differentiable & Other Methods
- **Sub-gradient Method** â€” Handles non-differentiable convex functions (e.g., Lasso regression).  
- **Line Search Techniques** â€” Used to determine optimal step size:
  - **Armijo Condition**
  - **Backtracking Line Search**

### 7. Regression_Models
- **Linear regression**  
  - **Baseline supervised learning model for continuous prediction.**
  - **Dataset Used: California Housing**
- **Ridge Regression (L2)**
  - **linear regression with L2 penalty to reduce overfitting.**
  - **Dataset Used: California Housing**
- **Lasso Regression (L1)**
  - **Adds an L1 penalty that increases sparsity**
  - **Dataset Used: California Housing**
- **Logistic Regression**
  - **Classification using sigmoid function.**
  - **Dataset Used: Breast Cancer Wisconsin Dataset**

---

## Project Structure

```bash
Optimization_Project/
â”œâ”€â”€ optimizers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gradient_descent.py         # GD, SGD, Mini-Batch
â”‚   â”œâ”€â”€ line_search.py              # Armijo, Backtracking
â”‚   â”œâ”€â”€ momentum.py                 # Momentum, Nesterov
â”‚   â”œâ”€â”€ adaptive.py                 # Adagrad, RMSProp, Adam
â”‚   â”œâ”€â”€ second_order.py             # Newton, Damped Newton
â”‚   â”œâ”€â”€ quasi_newton.py             # BFGS, L-BFGS
â”‚   â””â”€â”€ non_differentiable.py       # Sub-gradient Method
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_Gradient_Descent.ipynb       # Demos for GD, SGD
â”‚   â”œâ”€â”€ 02_Momentum_Methods.ipynb       # Demos for Momentum, NAG
â”‚   â”œâ”€â”€ 03_Adaptive_Methods.ipynb       # Demos for Adagrad, RMSProp, Adam
â”‚   â”œâ”€â”€ 04_Second_Order_Methods.ipynb   # Demos for Newton
â”‚   â”œâ”€â”€ 05_Quasi_Newton_Methods.ipynb   # Demos for BFGS, L-BFGS
â”‚   â””â”€â”€ 06_Subgradient_Method.ipynb     # Demo for Lasso
â”‚   â””â”€â”€ Regression_Models.ipynb 
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_functions.py               # Defines test functions and  gradients
â”‚   â””â”€â”€ plot_helpers.py                 # Common plotting utilities
â”‚
â”œâ”€â”€ requirements.txt                    # Dependencies
â””â”€â”€ README.md                           # You are here
```

---

## Setup and Installation

Follow these steps to set up the project locally:

### 1. Clone the Repository
```bash
git clone https://github.com/your-username/Optimization_Project.git
cd Optimization_Project
```

### 2. Create a Virtual Environment
Itâ€™s recommended to use a virtual environment for dependency management.

**On macOS/Linux**
```bash
python3 -m venv venv
```

**On Windows**
```bash
python -m venv venv
```

### 3. Activate the Virtual Environment

**On macOS/Linux**
```bash
source venv/bin/activate
```

**On Windows**
```bash
.env\Scriptsctivate
```

### 4. Install Required Libraries
Install dependencies from the `requirements.txt` file:
```bash
pip install -r requirements.txt
```

**Contents of `requirements.txt`:**
```
numpy
matplotlib
jupyter
scipy
```

> ðŸ’¡ *Note:* `scipy` is included for benchmarking your implementations against `scipy.optimize`.

---

## Running the Experiments

All experiments and visualizations are available in **Jupyter Notebooks**.

### Step 1. Start the Jupyter Notebook Server
```bash
jupyter notebook
```
A new browser tab will open automatically.

### Step 2. Open the Notebooks
Navigate to the `notebooks/` folder in the Jupyter interface.

### Step 3. Run Experiments
Open any notebook (e.g., `01_Gradient_Descent.ipynb`) and execute cells using:
```
Shift + Enter
```

Youâ€™ll see:
- Test function setup  
- Optimizer import from the `optimizers/` module  
- 2D contour visualizations of convergence paths  
- Comparisons between algorithms (e.g., GD vs. Momentum)  

---

## Example Visualization

Each notebook provides contour plots and convergence traces like:

- Gradient paths toward minima  
- Comparison of learning rate effects  
- Momentum trajectory smoothing  
- Adaptive methods vs. fixed learning rates  

---

## License

This project is developed for educational purposes as part of an academic semester project.  
Feel free to use or adapt for learning and research purposes with proper credit.

---
